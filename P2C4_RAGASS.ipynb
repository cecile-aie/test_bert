{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "authorship_tag": "ABX9TyO/te+Jz7duguTPgTCIfUqh"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": "!pip install \"mistralai>=1.0\" --quiet\n!pip install \"ragas>=0.4\" --quiet\n!pip install datasets --quiet\n!pip install langchain-mistralai --quiet",
   "metadata": {
    "id": "XOiH4xKYxThe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Imports et configuration\nimport os\nimport pandas as pd\nfrom typing import List, Dict, Optional\n\nfrom ragas import EvaluationDataset, SingleTurnSample, evaluate\nfrom ragas.metrics import (\n    Faithfulness,\n    ResponseRelevancy,\n    LLMContextPrecisionWithReference,\n    LLMContextRecall,\n)\nfrom ragas.llms import LangchainLLMWrapper\nfrom ragas.embeddings import LangchainEmbeddingsWrapper\nfrom langchain_mistralai.chat_models import ChatMistralAI\nfrom langchain_mistralai.embeddings import MistralAIEmbeddings",
   "metadata": {
    "id": "oN0J4Y9c4Lco"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# --- Fonctions utilitaires réutilisables ---\n\ndef build_evaluation_samples(\n    questions: List[str],\n    answers: List[str],\n    contexts: List[List[str]],\n    references: List[str],\n) -> List[SingleTurnSample]:\n    \"\"\"Construit une liste de SingleTurnSample à partir des données brutes.\n\n    Args:\n        questions: Liste des questions posées.\n        answers: Liste des réponses générées par le RAG.\n        contexts: Liste de listes de contextes récupérés pour chaque question.\n        references: Liste des réponses de référence (ground truth).\n\n    Returns:\n        Liste de SingleTurnSample prêts pour l'évaluation RAGAS.\n    \"\"\"\n    if not (len(questions) == len(answers) == len(contexts) == len(references)):\n        raise ValueError(\"Toutes les listes doivent avoir la même longueur.\")\n\n    return [\n        SingleTurnSample(\n            user_input=q,\n            response=a,\n            retrieved_contexts=c,\n            reference=r,\n        )\n        for q, a, c, r in zip(questions, answers, contexts, references)\n    ]\n\n\ndef create_evaluation_dataset(\n    questions: List[str],\n    answers: List[str],\n    contexts: List[List[str]],\n    references: List[str],\n) -> EvaluationDataset:\n    \"\"\"Crée un EvaluationDataset RAGAS à partir des données brutes.\n\n    Args:\n        questions: Liste des questions posées.\n        answers: Liste des réponses générées par le RAG.\n        contexts: Liste de listes de contextes récupérés pour chaque question.\n        references: Liste des réponses de référence (ground truth).\n\n    Returns:\n        EvaluationDataset prêt pour l'évaluation.\n    \"\"\"\n    samples = build_evaluation_samples(questions, answers, contexts, references)\n    return EvaluationDataset(samples=samples)\n\n\ndef init_mistral_evaluator(\n    api_key: str,\n    model: str = \"mistral-large-latest\",\n    temperature: float = 0.1,\n) -> tuple:\n    \"\"\"Initialise le LLM et les embeddings Mistral pour l'évaluation RAGAS.\n\n    Args:\n        api_key: Clé API Mistral.\n        model: Nom du modèle Mistral à utiliser.\n        temperature: Température de génération.\n\n    Returns:\n        Tuple (llm_wrapper, embeddings_wrapper) compatibles avec RAGAS.\n    \"\"\"\n    llm = ChatMistralAI(\n        mistral_api_key=api_key,\n        model=model,\n        temperature=temperature,\n    )\n    embeddings = MistralAIEmbeddings(mistral_api_key=api_key)\n\n    return LangchainLLMWrapper(llm), LangchainEmbeddingsWrapper(embeddings)\n\n\ndef run_ragas_evaluation(\n    dataset: EvaluationDataset,\n    llm,\n    embeddings,\n    metrics: Optional[list] = None,\n) -> pd.DataFrame:\n    \"\"\"Lance l'évaluation RAGAS et retourne les résultats sous forme de DataFrame.\n\n    Args:\n        dataset: EvaluationDataset contenant les échantillons.\n        llm: Instance LLM wrappée pour RAGAS.\n        embeddings: Instance embeddings wrappée pour RAGAS.\n        metrics: Liste de métriques RAGAS à évaluer.\n                 Par défaut: Faithfulness, ResponseRelevancy,\n                 LLMContextPrecisionWithReference, LLMContextRecall.\n\n    Returns:\n        DataFrame pandas avec les scores par échantillon.\n    \"\"\"\n    if metrics is None:\n        metrics = [\n            Faithfulness(),\n            ResponseRelevancy(),\n            LLMContextPrecisionWithReference(),\n            LLMContextRecall(),\n        ]\n\n    results = evaluate(\n        dataset=dataset,\n        metrics=metrics,\n        llm=llm,\n        embeddings=embeddings,\n    )\n    return results.to_pandas()\n\n\nprint(\"Fonctions utilitaires chargées.\")",
   "metadata": {
    "id": "XSKymHn_zaFm"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T58jahBBeHEz"
   },
   "outputs": [],
   "source": "# --- Données de test ---\n\nquestions_test = [\n    \"Quel est le nom du Maire ?\",\n    \"Quels ont été les principaux projets en 2023 ?\",\n    \"Quelles sont les horaires d'ouverture de la mairie ?\",\n    \"Résume moi le règlement municipal \",\n    \"Combien de km de pistes cyclables allons nous disposer ?\",\n    \"Bonjour, quel temps fait-il aujourd'hui ?\"\n]\n\nanswers = [\n    \"Le nom du Maire est Madame Pétillante Rigolade.\",\n    \"Rénovation de la voirie centrale : Refonte et réhabilitation des rues principales du centre-ville (lancé le 2023-09-01, budget de 750 000 €).Modernisation de l'éclairage public : Remplacement des lampadaires défectueux et installation de LED (lancé le 2023-10-15, budget de 300 000 €).Amélioration des espaces verts : Réaménagement des parcs et jardins municipaux (lancé le 2023-11-10, budget de 450 000 €).Rénovation de l'école primaire : Modernisation et extension des locaux de l'école primaire de la commune (lancé le 2023-08-20, budget de 650 000 €). Création du Pôle Sportif Municipal : Construction d'un complexe sportif multifonctionnel (lancé le 2023-07-15, budget de 950 000 €).Réaménagement de la place du Marché : Reconfiguration de l'espace public pour dynamiser le commerce local (lancé le 2023-10-05, budget de 500 000 €).Installation d'un système de surveillance urbaine : Mise en place de caméras pour renforcer la sécurité publique (lancé le 2023-12-01, budget de 400 000 €).Développement de pistes cyclables : Création d'un réseau sécurisé de pistes cyclables dans toute la commune (lancé le 2023-09-15, budget de 350 000 €).Mise en place d'un système de tri sélectif avancé : Optimisation de la gestion des déchets par un système de tri intelligent (lancé le 2023-11-20, budget de 300 000 €).\",\n    \"La mairie de Triffouillis-sur-Loire est ouverte du lundi au vendredi de 8h30 à 12h00. Elle est fermée le samedi et le dimanche. \",\n    \"Le règlement vise à garantir l'ordre public, la sécurité, la tranquillité et la salubrité sur l'ensemble du territoire communal. Il s'applique à tous les habitants, visiteurs et usagers de la commune.\",\n    \"Le plan d'action 2026 vise à créer environ 2,5 km de nouveaux aménagements cyclables sécurisés.\",\n    \"Bonjour ! Je ne peux pas fournir des informations en temps réel sur la météo. Pour connaître le temps qu'il fait aujourd'hui à Triffouillis-sur-Loire, je vous recommande de consulter un site ou une application météo fiable.\"\n]\n\ncontexts = [\n    [\"Extrait du document municipal page 5 : Le Conseil Municipal a élu Madame Pétillante Rigolade comme Maire lors de la séance du 15 juin.\"],\n    [\"Rapport d'activité 2023, section Travaux : Rénovation voirie centrale (750k€), Éclairage public LED (300k€), Réaménagement parcs (450k€).\", \"Annexe budgétaire 2023 : École primaire (650k€), Pôle Sportif (950k€), Place du Marché (500k€), Caméras surveillance (400k€), Pistes cyclables (350k€), Tri sélectif (300k€).\"],\n    [\"Page 'Infos Pratiques' du site web : Horaires Mairie : Lundi au Vendredi : 8h30 - 12h00. Samedi, Dimanche : Fermé.\"],\n    [\"Article 1 du Règlement Municipal : Le présent règlement a pour objet d'assurer le bon ordre, la sûreté, la sécurité, la tranquillité et la salubrité publiques sur le territoire de la commune.\", \"Article 2 : Il s'applique à toute personne se trouvant sur le territoire communal.\"],\n    [\"Plan Mobilité Douce 2022-2026, page 12 : Objectif 2026 : +2.5 km d'aménagements cyclables sécurisés (pistes et bandes).\"],\n    [\"Documentation interne de l'agent conversationnel : Limites connues : Ne pas fournir d'informations en temps réel comme la météo, la bourse, ou le trafic routier. Suggérer des sources externes.\"]\n]\n\nreferences = [\n    \"Le nom du Maire est Madame Pétillante Rigolade.\",\n    \"Les principaux projets en 2023 concernent la voirie, l'éclairage public, les espaces verts, l'école primaire, un pôle sportif, la place du marché, la surveillance urbaine, les pistes cyclables et le tri sélectif.\",\n    \"La mairie est ouverte du lundi au vendredi de 8h30 à 12h00.\",\n    \"Le règlement municipal vise principalement à garantir l'ordre public, la sécurité, la tranquillité et la salubrité dans la commune pour tous.\",\n    \"Le plan d'action 2026 prévoit environ 2,5 km de nouvelles pistes cyclables sécurisées.\",\n    \"Le système ne peut pas donner la météo en temps réel et suggère de consulter une source externe.\"\n]\n\n# Création du dataset d'évaluation\nevaluation_dataset = create_evaluation_dataset(\n    questions=questions_test,\n    answers=answers,\n    contexts=contexts,\n    references=references,\n)\nprint(f\"Dataset créé avec {len(evaluation_dataset)} échantillons.\")"
  },
  {
   "cell_type": "code",
   "source": "# --- Exécution de l'évaluation RAGAS ---\n\nMISTRAL_API_KEY = \"StthE16bt5IReMHPxgjbwtQLGggjOGtq\"\n\ntry:\n    # Initialisation du LLM et des embeddings\n    print(\"Initialisation de l'évaluateur Mistral...\")\n    evaluator_llm, evaluator_embeddings = init_mistral_evaluator(\n        api_key=MISTRAL_API_KEY,\n    )\n    print(\"LLM et embeddings initialisés.\")\n\n    # Lancement de l'évaluation\n    print(\"\\nLancement de l'évaluation RAGAS...\")\n    results_df = run_ragas_evaluation(\n        dataset=evaluation_dataset,\n        llm=evaluator_llm,\n        embeddings=evaluator_embeddings,\n    )\n\n    print(\"\\n--- Évaluation terminée ---\")\n\n    # Affichage des résultats détaillés par échantillon\n    pd.set_option(\"display.max_columns\", None)\n    pd.set_option(\"display.max_colwidth\", 80)\n    print(\"\\n--- Résultats par échantillon ---\")\n    display(results_df)\n\n    # Affichage des scores moyens par métrique\n    metric_cols = [c for c in results_df.columns if c not in (\"user_input\", \"response\", \"retrieved_contexts\", \"reference\")]\n    print(\"\\n--- Scores moyens ---\")\n    for col in metric_cols:\n        print(f\"  {col}: {results_df[col].mean():.4f}\")\n\nexcept Exception as e:\n    print(f\"\\nErreur lors de l'évaluation : {e}\")\n    import traceback\n    traceback.print_exc()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}